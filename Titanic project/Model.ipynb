{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "134ba4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../Titanic project/input/train.csv\")\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "71c14b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            137\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          552\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()\n",
    "# X_valid.isnull().sum()\n",
    "# print(X_train)\n",
    "\n",
    "# df[['Age', 'Cabin', 'Embarked']].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "55aed3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             40\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          135\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1754567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Column  Outlier Count  Lower Bound  Upper Bound  Outlier %\n",
      "0  PassengerId              0      -444.00      1336.00       0.00\n",
      "1       Pclass              0         0.50         4.50       0.00\n",
      "2          Age             11        -6.69        64.81       1.23\n",
      "3        SibSp             46        -1.50         2.50       5.16\n",
      "4        Parch            213         0.00         0.00      23.91\n",
      "5         Fare            116       -26.72        65.63      13.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_outliers_iqr(df, col, factor=1.5):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "# L·∫•y danh s√°ch c√°c c·ªôt s·ªë\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# T·∫°o DataFrame ƒë·ªÉ th·ªëng k√™ outlier\n",
    "outlier_summary = []\n",
    "\n",
    "for col in num_cols:\n",
    "    n_outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outlier Count': n_outliers,\n",
    "        'Lower Bound': round(lower, 2),\n",
    "        'Upper Bound': round(upper, 2),\n",
    "        'Outlier %': round(n_outliers / len(df) * 100, 2)\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2d5aabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(df, mean_age=None, mode_embarked=None, fit_encoders=False, encoders=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    # N·∫øu ch∆∞a truy·ªÅn mean/mode, t√≠nh t·ª´ df (th∆∞·ªùng l√† train)\n",
    "    if mean_age is None:\n",
    "        mean_age = df['Age'].mean()\n",
    "    if mode_embarked is None:\n",
    "        mode_embarked = df['Embarked'].mode()[0]\n",
    "\n",
    "    # ƒêi·ªÅn gi√° tr·ªã thi·∫øu\n",
    "    df['Age'] = df['Age'].fillna(mean_age)\n",
    "    df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "\n",
    "    # X·ª≠ l√Ω outlier\n",
    "    df['Age'] = df['Age'].clip(0, 65)\n",
    "    df['SibSp'] = df['SibSp'].clip(0, 5)\n",
    "    df['Parch'] = df['Parch'].clip(0, 4)\n",
    "    df['Fare'] = np.log1p(df['Fare'])\n",
    "\n",
    "    # --- M√É H√ìA ---\n",
    "\n",
    "    # 1Ô∏èPh√¢n nh√≥m tu·ªïi (Age bins)\n",
    "    bins = [0, 12, 18, 35, 60, 100]\n",
    "    labels = ['Child', 'Teen', 'Adult', 'MidAge', 'Senior']\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "\n",
    "    # 2Ô∏èOne-hot encode cho bi·∫øn ph√¢n lo·∫°i\n",
    "    df = pd.get_dummies(df, columns=['Sex', 'Embarked', 'AgeGroup'], drop_first=True)\n",
    "    df = df.drop(['Cabin', 'Age','PassengerId', 'Name', 'Ticket','SibSp', 'Parch'], axis=1)\n",
    "\n",
    "    return df, mean_age, mode_embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2bed6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep, mean_age, mode_embarked = preprocess(X_train)\n",
    "X_valid_prep, _, _ = preprocess(X_valid, mean_age, mode_embarked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "6961dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass      Fare  Sex_male  Embarked_Q  Embarked_S  AgeGroup_Teen  \\\n",
      "565       3  3.224858      True       False        True          False   \n",
      "160       3  2.839078      True       False        True          False   \n",
      "553       3  2.107178      True       False       False          False   \n",
      "860       3  2.715244      True       False        True          False   \n",
      "241       3  2.803360     False        True       False          False   \n",
      "..      ...       ...       ...         ...         ...            ...   \n",
      "880       2  3.295837     False       False        True          False   \n",
      "91        3  2.180892      True       False        True          False   \n",
      "883       2  2.442347      True       False        True          False   \n",
      "473       2  2.694066     False       False       False          False   \n",
      "637       2  3.305054      True       False        True          False   \n",
      "\n",
      "     AgeGroup_Adult  AgeGroup_MidAge  AgeGroup_Senior  \n",
      "565            True            False            False  \n",
      "160           False             True            False  \n",
      "553            True            False            False  \n",
      "860           False             True            False  \n",
      "241            True            False            False  \n",
      "..              ...              ...              ...  \n",
      "880            True            False            False  \n",
      "91             True            False            False  \n",
      "883            True            False            False  \n",
      "473            True            False            False  \n",
      "637            True            False            False  \n",
      "\n",
      "[179 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_valid_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "851000e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       110\n",
      "           1       0.71      0.68      0.70        69\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.76      0.75      0.76       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìò Decision Tree Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83       110\n",
      "           1       0.74      0.67      0.70        69\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.76      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìò Random Forest Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       110\n",
      "           1       0.81      0.72      0.76        69\n",
      "\n",
      "    accuracy                           0.83       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.83      0.83      0.82       179\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìò KNN Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       110\n",
      "           1       0.77      0.64      0.70        69\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.76      0.77       179\n",
      "weighted avg       0.79      0.79      0.78       179\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìò XGBoost Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85       110\n",
      "           1       0.77      0.72      0.75        69\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.80      0.79      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "------------------------------------------------------------\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "2        Random Forest  0.826816   0.806452  0.724638  0.763359\n",
      "4              XGBoost  0.810056   0.769231  0.724638  0.746269\n",
      "1        Decision Tree  0.782123   0.741935  0.666667  0.702290\n",
      "3                  KNN  0.787709   0.771930  0.637681  0.698413\n",
      "0  Logistic Regression  0.770950   0.712121  0.681159  0.696296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# --- T√°ch d·ªØ li·ªáu n·∫øu ch∆∞a c√≥ ---\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Danh s√°ch m√¥ h√¨nh ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# --- Hu·∫•n luy·ªán v√† ƒë√°nh gi√° ---\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_prep, y_train)\n",
    "    preds = model.predict(X_valid_prep)\n",
    "    \n",
    "    acc = accuracy_score(y_valid, preds)\n",
    "    prec = precision_score(y_valid, preds)\n",
    "    rec = recall_score(y_valid, preds)\n",
    "    f1 = f1_score(y_valid, preds)\n",
    "    \n",
    "    results.append((name, acc, prec, rec, f1))\n",
    "    print(f\"üìò {name} Report:\\n{classification_report(y_valid, preds)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# --- T·ªïng h·ª£p k·∫øt qu·∫£ ---\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "results_df = results_df.sort_values(by=\"F1-Score\", ascending=False)\n",
    "# print(\"\\nüìä T·ªïng h·ª£p k·∫øt qu·∫£:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "30a2a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "‚úÖ Best Params: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "‚úÖ Best F1-score: 0.7390793424690789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# B·ªô tham s·ªë c·∫ßn th·ª≠\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# T·∫°o model v√† grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',   # c√≥ th·ªÉ ƒë·ªïi th√†nh 'accuracy' ho·∫∑c 'recall'\n",
    "    cv=5,           # 5-fold cross-validation\n",
    "    n_jobs=-1,      # t·∫≠n d·ª•ng to√†n b·ªô CPU\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_prep, y_train)\n",
    "\n",
    "# In ra k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    "print(\"‚úÖ Best Params:\", grid_search.best_params_)\n",
    "print(\"‚úÖ Best F1-score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "57b66431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "‚úÖ Best Params: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 12, 'bootstrap': False}\n",
      "‚úÖ Best F1-score (CV): 0.7566083203212021\n",
      "\n",
      "üìä Classification Report (Validation set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       110\n",
      "           1       0.78      0.77      0.77        69\n",
      "\n",
      "    accuracy                           0.83       179\n",
      "   macro avg       0.82      0.82      0.82       179\n",
      "weighted avg       0.83      0.83      0.83       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# === 1Ô∏è‚É£ ƒê·ªãnh nghƒ©a grid c·∫ßn th·ª≠ ===\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500],       # s·ªë c√¢y\n",
    "    'max_depth': [None, 5, 8, 12, 15, 20],          # ƒë·ªô s√¢u c√¢y\n",
    "    'min_samples_split': [2, 5, 10],                # min m·∫´u ƒë·ªÉ chia node\n",
    "    'min_samples_leaf': [1, 2, 4],                  # min m·∫´u ·ªü l√°\n",
    "    'max_features': ['sqrt', 'log2'],               # s·ªë feature x√©t khi chia\n",
    "    'bootstrap': [True, False]                      # c√≥ d√πng bootstrap kh√¥ng\n",
    "}\n",
    "\n",
    "# === 2Ô∏è‚É£ T·∫°o model g·ªëc ===\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# === 3Ô∏è‚É£ D√πng RandomizedSearchCV ===\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,             # th·ª≠ 30 t·ªï h·ª£p ng·∫´u nhi√™n\n",
    "    scoring='f1',          # t·ªëi ∆∞u theo F1\n",
    "    cv=5,                  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # t·∫≠n d·ª•ng to√†n b·ªô CPU\n",
    ")\n",
    "\n",
    "# === 4Ô∏è‚É£ Train ===\n",
    "random_search.fit(X_train_prep, y_train)\n",
    "\n",
    "# === 5Ô∏è‚É£ K·∫øt qu·∫£ ===\n",
    "print(\"‚úÖ Best Params:\", random_search.best_params_)\n",
    "print(\"‚úÖ Best F1-score (CV):\", random_search.best_score_)\n",
    "\n",
    "# === 6Ô∏è‚É£ ƒê√°nh gi√° l·∫°i tr√™n t·∫≠p validation ===\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_valid_prep)\n",
    "\n",
    "print(\"\\nüìä Classification Report (Validation set):\")\n",
    "print(classification_report(y_valid, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
