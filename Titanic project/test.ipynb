{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8654a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1-Score  Mean_CV_F1\n",
      "4              XGBoost  0.860335   0.845070  0.810811  0.827586    0.725880\n",
      "1        Random Forest  0.826816   0.811594  0.756757  0.783217    0.735135\n",
      "0  Logistic Regression  0.810056   0.777778  0.756757  0.767123    0.760594\n",
      "2        Decision Tree  0.782123   0.733333  0.743243  0.738255    0.721761\n",
      "3                  KNN  0.748603   0.723077  0.635135  0.676259    0.633657\n",
      "Step 1: Scaling features...\n",
      "Step 2: Feature selection (L1 penalty)...\n",
      "Selected 19 features from 24 original features.\n",
      "Step 3: Generating Polynomial Features (degree=2)...\n",
      "Polynomial features expanded to 209 features.\n",
      "Step 4: GridSearchCV for Logistic Regression hyperparameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Evaluating on validation set...\n",
      "\n",
      "=== Logistic Regression Optimization Results ===\n",
      "Best Params: {'C': 0.01, 'l1_ratio': 0, 'penalty': 'l2'}\n",
      "Best CV F1: 0.7661766834393332\n",
      "Validation Accuracy: 0.8044692737430168\n",
      "Validation F1: 0.75177304964539\n",
      "Precision: 0.7910447761194029\n",
      "Recall: 0.7162162162162162\n",
      "{'Best Params': {'C': 0.01, 'l1_ratio': 0, 'penalty': 'l2'}, 'Best CV F1': np.float64(0.7661766834393332), 'Validation Accuracy': 0.8044692737430168, 'Validation F1': 0.75177304964539, 'Precision': 0.7910447761194029, 'Recall': 0.7162162162162162}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# ------------------- Preprocess -------------------\n",
    "def preprocess(df, mean_age=None, mode_embarked=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    if mean_age is None:\n",
    "        mean_age = df['Age'].mean()\n",
    "    if mode_embarked is None:\n",
    "        mode_embarked = df['Embarked'].mode()[0]\n",
    "\n",
    "    df['Age'] = df['Age'].fillna(mean_age)\n",
    "    df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    df['Age'] = df['Age'].clip(0,65)\n",
    "    df['SibSp'] = df['SibSp'].clip(0,5)\n",
    "    df['Parch'] = df['Parch'].clip(0,4)\n",
    "    df['Fare'] = np.log1p(df['Fare'])\n",
    "\n",
    "    df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "    df['Title'] = df['Title'].replace(\n",
    "        ['Lady', 'Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "    df['Ticket_prefix'] = df['Ticket'].str.extract('([A-Za-z./]+)', expand=False)\n",
    "    df['Ticket_prefix'] = df['Ticket_prefix'].fillna('NONE')\n",
    "    rare_prefix = df['Ticket_prefix'].value_counts()[df['Ticket_prefix'].value_counts() < 10].index\n",
    "    df['Ticket_prefix'] = df['Ticket_prefix'].replace(rare_prefix, 'Rare')\n",
    "    df['Ticket_number'] = df['Ticket'].str.extract('(\\d+)', expand=False)\n",
    "    df['Ticket_number'] = df['Ticket_number'].fillna(0).astype(int)\n",
    "    df['Ticket_number'] = np.log1p(df['Ticket_number'])\n",
    "\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize']==1).astype(int)\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['Sex','Embarked','Title','Ticket_prefix'], drop_first=False)\n",
    "    df = df.drop(['PassengerId','Cabin','Name','Ticket'], axis=1)\n",
    "\n",
    "    return df, mean_age, mode_embarked\n",
    "\n",
    "# ------------------- Load data -------------------\n",
    "train_df = pd.read_csv(\"../Titanic project/input/train.csv\")\n",
    "y = train_df['Survived']\n",
    "X = train_df.drop('Survived', axis=1)\n",
    "# X_prep, mean_age, mode_embarked = preprocess(X)\n",
    "\n",
    "\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, mean_age, mode_embarked = preprocess(X_train_raw)\n",
    "\n",
    "X_valid, _, _ = preprocess(X_valid_raw, mean_age=mean_age, mode_embarked=mode_embarked)\n",
    "\n",
    "# ⚠ Fix cột thiếu ở validation (one-hot)\n",
    "for col in X_train.columns:\n",
    "    if col not in X_valid.columns:\n",
    "        X_valid[col] = 0\n",
    "X_valid = X_valid[X_train.columns]  # sắp xếp theo train\n",
    "# ------------------- Models -------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=3000, solver='saga', penalty='l2'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# ------------------- Train & Evaluate -------------------\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit trên train set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)    \n",
    "\n",
    "\n",
    "    if name == \"XGBoost\":\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        f1_scores = []\n",
    "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "            X_tr, X_va = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_va = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred_va = model.predict(X_va)\n",
    "            f1_scores.append(f1_score(y_va, y_pred_va))\n",
    "        mean_cv_f1 = np.mean(f1_scores)\n",
    "        acc = accuracy_score(y_valid, y_pred)\n",
    "        f1_val = f1_score(y_valid, y_pred)    \n",
    "        report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": report['1']['precision'],\n",
    "            \"Recall\": report['1']['recall'],\n",
    "            \"F1-Score\": f1_val,\n",
    "            \"Mean_CV_F1\": mean_cv_f1\n",
    "        })\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "        mean_cv_f1 = cv_scores.mean()\n",
    "        acc = accuracy_score(y_valid, y_pred)\n",
    "        f1_val = f1_score(y_valid, y_pred)    \n",
    "        report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": report['1']['precision'],\n",
    "            \"Recall\": report['1']['recall'],\n",
    "            \"F1-Score\": f1_val,\n",
    "            \"Mean_CV_F1\": mean_cv_f1\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by='F1-Score', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "def optimize_logistic_regression(X_train, y_train, X_valid, y_valid, use_polynomial=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimize Logistic Regression với:\n",
    "    - Scaling\n",
    "    - L1 feature selection\n",
    "    - Polynomial features (tùy chọn)\n",
    "    - GridSearchCV (tuning C, penalty, l1_ratio)\n",
    "    \n",
    "    Trả về dict với kết quả Best Params, CV F1, Validation Accuracy/F1/Precision/Recall\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose: print(\"Step 1: Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    if verbose: print(\"Step 2: Feature selection (L1 penalty)...\")\n",
    "    selector = SelectFromModel(\n",
    "        LogisticRegression(penalty='l1', solver='saga', C=1, max_iter=5000, random_state=42)\n",
    "    )\n",
    "    selector.fit(X_train_scaled, y_train)\n",
    "    X_train_sel = selector.transform(X_train_scaled)\n",
    "    X_valid_sel = selector.transform(X_valid_scaled)\n",
    "    if verbose: print(f\"Selected {X_train_sel.shape[1]} features from {X_train_scaled.shape[1]} original features.\")\n",
    "\n",
    "    if use_polynomial:\n",
    "        if verbose: print(\"Step 3: Generating Polynomial Features (degree=2)...\")\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "        X_train_poly = poly.fit_transform(X_train_sel)\n",
    "        X_valid_poly = poly.transform(X_valid_sel)\n",
    "        if verbose: print(f\"Polynomial features expanded to {X_train_poly.shape[1]} features.\")\n",
    "    else:\n",
    "        X_train_poly, X_valid_poly = X_train_sel, X_valid_sel\n",
    "\n",
    "    if verbose: print(\"Step 4: GridSearchCV for Logistic Regression hyperparameters...\")\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'l1_ratio': [0, 0.5, 1]  # chỉ dùng khi penalty='elasticnet'\n",
    "    }\n",
    "    logreg = LogisticRegression(max_iter=5000, solver='saga', random_state=42)\n",
    "    grid = GridSearchCV(logreg, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train_poly, y_train)\n",
    "\n",
    "    if verbose: print(\"Step 5: Evaluating on validation set...\")\n",
    "    y_pred = grid.predict(X_valid_poly)\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    f1_val = f1_score(y_valid, y_pred)\n",
    "    report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "\n",
    "    results = {\n",
    "        'Best Params': grid.best_params_,\n",
    "        'Best CV F1': grid.best_score_,\n",
    "        'Validation Accuracy': acc,\n",
    "        'Validation F1': f1_val,\n",
    "        'Precision': report['1']['precision'],\n",
    "        'Recall': report['1']['recall']\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n=== Logistic Regression Optimization Results ===\")\n",
    "        for k, v in results.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Giả sử bạn đã preprocess X_train và X_valid\n",
    "logreg_results = optimize_logistic_regression(X_train, y_train, X_valid, y_valid, use_polynomial=True)\n",
    "print(logreg_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
